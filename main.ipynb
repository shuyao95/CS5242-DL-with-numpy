{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "**ASSIGNMENT DEADLINE: 19 Oct 2019 17:00**\n",
    "\n",
    "In this assignemnt, the task is to implement some basic components for training convolutional neural network (CNN) over the MNIST dataset and recurrent neural network (RNN) over the NLTK dataset. You need to follow the lecture notes to \n",
    "- implement the RMSProp algorithm in nn/optimizers.py\n",
    "- implement multiple operations in nn/operators.py, which are used by the layers (in nn/layer.py)\n",
    "- implement the forward function of Bi-directional RNN (in nn/layer.py)\n",
    "- tune the model architecture and some hyperparameters to improve the accuracy.\n",
    "\n",
    "**Attention**:\n",
    "- To run this Jupyter notebook, you need to install the dependent libraries as stated in [README.MD](README.MD). You do not need and should not use other libraries (like tensorflow and pytorch) in your code. The major version of Python should be 3.\n",
    "- You do not need a GPU for this assignment. CPU is enough.\n",
    "- Before you finish the implementation of the required functions, if you run this notebook, you may see errors.\n",
    "- Do not change the signature (the name and arguments) of the existing functions in the repository ; otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- Do not change the structure of files in the repository (e.g., adding, renaming or deleting any files); otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- You can add functions in the existing files, but you should not change the import statements (e.g., adding a new import statement). For example, if you want to implement a function foo(), you can implement it inside operator.py and call it; but you cannot implement it in another file and import that file in operators.py. Otherwise your implementation cannot be tested correctly and you will get penalty.\n",
    "- After you implement one function, remember to restart the notebook kernel to help it recognize your fresh code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Structure of the Repository\n",
    "\n",
    "The structure of this repository is shown as below:\n",
    "\n",
    "```bash\n",
    "codes/\n",
    "    data/\n",
    "        datasets.py     # load dataset, like MNIST\n",
    "        mnist.npz       # mnist dataset \n",
    "        corpus.csv      # for nltk dataset \n",
    "        dictionary.csv  # for nltk dataset\n",
    "    models/             # example models of your tiny deep learning framework\n",
    "        MNISTNet.py     # example model on MNIST dataset\n",
    "        SentimentNet.py # example model on nltk dataset\n",
    "    nn/                 # components of neural networks\n",
    "        operators.py    # operators; **You need to edit this file to add missing code**\n",
    "        optimizers.py   # optimizing methods; **You need to implement the RMSprop algorithm**\n",
    "        layers.py       # layer abstract for CNN and RNN\n",
    "        loss.py         # loss function for optimization\n",
    "        model.py        # model abstraction for defining and training models\n",
    "        initializers.py # initializing methods to initialize parameters (like weights, bias)\n",
    "        funtional.py    # some helpful function during implementation of training\n",
    "    utils/              # some additional tools for CNN\n",
    "        check_grads_cnn.py  # for CNN, help you check your forward function and backward function\n",
    "        check_grads_rnn.py  # for RNN, help you check your forward function and backward function\n",
    "        tools.py        # other useful functions for testing the codes\n",
    "    main.ipynb          # this notebook which calls the functions in other modules/files\n",
    "    README.MD           # list of dependent libraries\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Functionality of This Notebook\n",
    "\n",
    "This iPython notebook serves to:\n",
    "\n",
    "- explain code structure, main APIs\n",
    "- explain your implementation task and tuning task\n",
    "- provide code to test your implemented forward and backward function for different operations\n",
    "- provide related materials to help you understand the implementation of some operations and optimizers\n",
    "\n",
    "*You can type `jupyter lab` in the terminal to start this jupyter notebook when your current working directory is cs5242. It's much more convinient than jupyter notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Your Task on CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Base Classes\n",
    "\n",
    "In [nn/optimizers.py](nn/optimizers.py), we define the base optimizer class. We have also implemented SGD, Adagrad, Adam for you. **You only need to implement the RMSprop optimizer in the [nn/optimizers.py](nn/optimizers.py) following the same style.**\n",
    "\n",
    "```python\n",
    "class Optimizer():\n",
    "\n",
    "    def __init__(self, lr):\n",
    "        \"\"\"Initialization\n",
    "\n",
    "        # Arguments\n",
    "            lr: float, learnig rate \n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, x, x_grad, iteration):\n",
    "        \"\"\"Update parameters with gradients\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sheduler(self, func, iteration):\n",
    "        \"\"\"learning rate sheduler, to change learning rate with respect to iteration\n",
    "\n",
    "        # Arguments\n",
    "            func: function, arguments are lr and iteration\n",
    "            iteration: int, current iteration number in the whole training process (not in that epoch)\n",
    "\n",
    "        # Returns\n",
    "            lr: float, the new learning rate\n",
    "        \"\"\"\n",
    "        lr = func(self.lr, iteration)\n",
    "        return lr\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In [nn/operators.py](nn/operators.py), we define the base operator class and have implemented some operations (like relu, flatten, linear) for you. **You only need to implement the rest operations in the [nn/operators.py](nn/operators.py) following the same style.\n",
    "\n",
    "```python\n",
    "class operator(object):\n",
    "    \"\"\"\n",
    "    operator abstraction\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Forward operation, reture output\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        \"\"\"Backward operation, return gradient to input\"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop Optimizer\n",
    "In the file [nn/optimizers.py](nn/optimizers.py), there are 4 types of optimizer (`SGD`, `Adam`, `RMSprop` and `Adagrad`). **You only need to implement the `update` function of `RMSprop`**. \n",
    "\n",
    "`RMSprop` optimizer is initialized like this:\n",
    "\n",
    "```python\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, lr=0.001, bata=0.9, epsilon=None, decay=0, sheduler_func=None):\n",
    "        super(RMSprop, self).__init__(lr)\n",
    "        self.bata = bata\n",
    "        self.epsilon = epsilon\n",
    "        self.decay = decay\n",
    "        if not self.epsilon:\n",
    "            self.epsilon = 1e-8\n",
    "        self.accumulators = None\n",
    "        self.sheduler_func = sheduler_func\n",
    "```\n",
    "\n",
    "- `lr`: The initial learning rate.\n",
    "- `beta`: The weight of moving average for second moment of gradient\n",
    "- `decay`: The learning rate decay ratio\n",
    "- `sheduler_func`: Function to change learning rate with respect to iterations\n",
    "\n",
    "More details can be found in the slides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covolution Layer\n",
    "\n",
    "Conv2D (in [nn/layers.py](nn/layers.py)) implements the Convolution layer. It maintains the weight matrix and bias vector, and calls the forward and backward funtion of `conv` class in [nn/operators.py](nn/operators.py) to do the real operations. \n",
    "\n",
    "\n",
    "```python\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, conv_params, initializer=Gaussian(), name='conv'):\n",
    "        super(Conv2D, self).__init__(name=name)\n",
    "        self.conv_params = conv_params\n",
    "        self.conv = conv(conv_params)\n",
    "\n",
    "        self.trainable = True\n",
    "\n",
    "        self.weights = initializer.initialize(\n",
    "            (conv_params['out_channel'], conv_params['in_channel'], conv_params['kernel_h'], conv_params['kernel_w']))\n",
    "        self.bias = np.zeros((conv_params['out_channel']))\n",
    "\n",
    "        self.w_grad = np.zeros(self.weights.shape)\n",
    "        self.b_grad = np.zeros(self.bias.shape)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv.forward(input, self.weights, self.bias)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad, self.w_grad, self.b_grad = self.conv.backward(\n",
    "            out_grad, input, self.weights, self.bias)\n",
    "        return in_grad\n",
    "\n",
    "```\n",
    "\n",
    "`conv_params` is a dictionary, containing these parameters:\n",
    "\n",
    "- 'kernel_h': The height of kernel.\n",
    "- 'kernel_w': The width of kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels padded to the bottom, top, left and right of each feature map. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n",
    "- 'in_channel': The number of input channels.\n",
    "- 'out_channel': The number of output channels.\n",
    "\n",
    "`initializer` is an instance of Initializer class, used to initialize parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function of Conv Operator\n",
    "\n",
    "You need to implement the forward function of the `conv` class in [nn/operators.py](nn/operators.py).\n",
    "\n",
    "The input consists of N data points, each with C channels, height H and width W. We convolve each input with K different kernels, where each kernel has  C channels and has height HH and width WW.\n",
    "\n",
    "**WARNING: Please implement the matrix product (img2col) method as shown in the Lecture notes. The naive  implementation is too slow for training a complete CNN model.**\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error (<1e-6 will be fine):  3.349876505161958e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from nn.layers import Conv2D\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Conv2D as k_Conv2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'kernel_h': 5,\n",
    "    'kernel_w': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': input.shape[1],\n",
    "    'out_channel': 64,\n",
    "}\n",
    "conv = Conv2D(params)\n",
    "out = conv.forward(input)\n",
    "\n",
    "keras_conv = Sequential([\n",
    "    k_Conv2D(filters=params['out_channel'],\n",
    "            kernel_size=(params['kernel_h'], params['kernel_w']),\n",
    "            strides=(params['stride'], params['stride']),\n",
    "            padding='valid',\n",
    "            data_format='channels_first',\n",
    "            input_shape=input.shape[1:]),\n",
    "])\n",
    "keras_conv.layers[0].set_weights([conv.weights.transpose((2,3,1,0)), conv.bias])\n",
    "\n",
    "keras_out = keras_conv.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function of Conv Operator\n",
    " \n",
    "You need to implement the backward function for the `conv` class in the file [nn/operators.py](nn/operators.py). \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following to check your backward implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n",
      "Gradient to weights:  correct\n",
      "Gradient to bias:  correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Conv2D\n",
    "import numpy as np\n",
    "from utils.check_grads_cnn import check_grads_layer\n",
    "\n",
    "batch = 10\n",
    "conv_params={\n",
    "    'kernel_h': 3,\n",
    "    'kernel_w': 3,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "    'in_channel': 3,\n",
    "    'out_channel': 10\n",
    "}\n",
    "in_height = 10\n",
    "in_width = 20\n",
    "out_height = 1+(in_height+2*conv_params['pad']-conv_params['kernel_h'])//conv_params['stride']\n",
    "out_width = 1+(in_width+2*conv_params['pad']-conv_params['kernel_w'])//conv_params['stride']\n",
    "\n",
    "input = np.random.uniform(size=(batch, conv_params['in_channel'], in_height, in_width))\n",
    "out_grad = np.random.uniform(size=(batch, conv_params['out_channel'], out_height, out_width))\n",
    "\n",
    "conv = Conv2D(conv_params)\n",
    "check_grads_layer(conv, input, out_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer\n",
    "\n",
    "Pool2D (in nn/layers.py) implements the pooling layer. It calls the forward and backward funtion of the pool class in nn/operators.py to do the real operations. \n",
    "\n",
    "Ihe initialization, forward and backward funtion of the class `Pooling` are shown as below:\n",
    "\n",
    "```python\n",
    "class Pool2D(Layer):\n",
    "    def __init__(self, pool_params, name='pooling'):\n",
    "        super(Pool2D, self).__init__(name=name)\n",
    "        self.pool_params = pool_params\n",
    "        self.pool = pool(pool_params)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.pool.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.pool.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "`pool_params` is a dictionary, containing these parameters:\n",
    "- 'pool_type': The type of pooling, 'max' or 'avg'\n",
    "- 'pool_h': The height of pooling kernel.\n",
    "- 'pool_w': The width of pooling kernel.\n",
    "- 'stride': The number of pixels between adjacent receptive fields in the horizontal and vertical directions.\n",
    "- 'pad': The number of pixels that will be used to zero-pad the input in each x-y direction. **Here pad=2 means adding 2 zeros to the left, right, top and bottom respectively**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function of Pool Operator\n",
    "\n",
    "You need to implement the forward function for `pool` class in the file [nn/operators.py](nn/operators.py).\n",
    "\n",
    "You can test your implementation by restarting jupyter notebook kernel and running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error (<1e-6 will be fine):  7.849888966570502e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nn.layers import Pool2D\n",
    "from utils.tools import rel_error\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "input = np.random.uniform(size=(10, 3, 30, 30))\n",
    "params = { \n",
    "    'pool_type': 'max',\n",
    "    'pool_height': 5,\n",
    "    'pool_width': 5,\n",
    "    'pad': 0,\n",
    "    'stride': 2,\n",
    "}\n",
    "pool = Pool2D(params)\n",
    "out = pool.forward(input)\n",
    "\n",
    "keras_pool = Sequential([\n",
    "    MaxPooling2D(pool_size=(params['pool_height'], params['pool_width']),\n",
    "                 strides=params['stride'],\n",
    "                 padding='valid',\n",
    "                 data_format='channels_first',\n",
    "                 input_shape=input.shape[1:])\n",
    "])\n",
    "keras_out = keras_pool.predict(input, batch_size=input.shape[0])\n",
    "print('Relative error (<1e-6 will be fine): ', rel_error(out, keras_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer\n",
    "\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some neurons to zero. \n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012\n",
    "\n",
    "\n",
    "The Dropout layer calls the `dropout` operator in [nn/operators.py](nn/operators.py) to do the real operations.\n",
    "\n",
    "```python\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate, name='dropout', seed=None):\n",
    "        super(Dropout, self).__init__(name=name)\n",
    "        self.rate = rate\n",
    "        self.seed = seed\n",
    "        self.dropout = dropout(rate, self.training, seed)  # create the dropout operator instance\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.dropout.forward(input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, out_grad, input):\n",
    "        in_grad = self.dropout.backward(out_grad, input)\n",
    "        return in_grad\n",
    "```\n",
    "\n",
    "- `rate`: The probability of setting a neuron to zero\n",
    "- `seed`: seed: int, random seed to sample from input, so as to get mask, which is convenient to check gradients. But for real training, it should be None to make sure to randomly drop neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function of Dropout Operator\n",
    "\n",
    "In [nn/operators.py](nn/operators.py), you need to implement the forward function for `dropout` class. Since dropout behaves differently during training and testing, you need to consider both modes.  `p` refers to the probability of setting a neuron to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function of Dropout Operator\n",
    "\n",
    "In [nn/operatiors.py](nn/operators.py), you need to implement the backward function for `dropout` class. After the implementation, restart jupyter notebook and run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient to input: correct\n"
     ]
    }
   ],
   "source": [
    "from nn.layers import Dropout\n",
    "\n",
    "import numpy as np\n",
    "from utils.check_grads_cnn import check_grads_layer\n",
    "\n",
    "rate = 0.1\n",
    "batch = 2\n",
    "height = 10\n",
    "width = 20\n",
    "channel = 10\n",
    "\n",
    "np.random.seed(1234)\n",
    "input = np.random.uniform(size=(batch, channel, height, width))\n",
    "out_grads = np.random.uniform(size=(batch, channel, height, width))\n",
    "\n",
    "dropout = Dropout(rate, seed=1234)\n",
    "dropout.set_mode(True)\n",
    "check_grads_layer(dropout, input, out_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using CNNs\n",
    "In this section, you are required to test your implementations above by running an ensemble NN on a MNIST dataset. The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "We have provided you with [models/MNISTNet.py](models/MNISTNet.py), which defines a CNN model for this task. By training the `MNISTNet` for one epoch, you should achieve above 85% on test set. You may have to wait about 5 minutes for training to be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  48000\n",
      "Number of validation images:  12000\n",
      "Number of testing images:  10000\n",
      "\n",
      "Four examples of training images:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb6c1d95f10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1296 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from models.MNISTNet import MNISTNet\n",
    "from nn.loss import SoftmaxCrossEntropy, L2\n",
    "from nn.optimizers import Adam\n",
    "from data.datasets import MNIST\n",
    "np.random.seed(5242)\n",
    "\n",
    "mnist = MNIST()\n",
    "mnist.load()\n",
    "idx = np.random.randint(mnist.num_train, size=4)\n",
    "print('\\nFour examples of training images:')\n",
    "img = mnist.x_train[idx][:,0,:,:]\n",
    "\n",
    "plt.figure(1, figsize=(18, 18))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(img[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(img[1])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(img[2])\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(img[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n",
      "Train iter 100/960:\tacc 0.10, loss 2.30, reg loss 0.00, speed 1093.39 samples/sec\n",
      "Train iter 200/960:\tacc 0.12, loss 2.27, reg loss 0.00, speed 1118.23 samples/sec\n",
      "Train iter 300/960:\tacc 0.68, loss 1.03, reg loss 0.02, speed 1119.61 samples/sec\n",
      "Train iter 400/960:\tacc 0.78, loss 0.58, reg loss 0.02, speed 1132.12 samples/sec\n",
      "Train iter 500/960:\tacc 0.88, loss 0.63, reg loss 0.02, speed 1114.30 samples/sec\n",
      "Train iter 600/960:\tacc 0.80, loss 0.58, reg loss 0.02, speed 1139.60 samples/sec\n"
     ]
    }
   ],
   "source": [
    "from nn.optimizers import RMSprop, Adam\n",
    "\n",
    "model = MNISTNet()\n",
    "loss = SoftmaxCrossEntropy(num_class=10)\n",
    "\n",
    "# define your learning rate sheduler\n",
    "def func(lr, iteration):\n",
    "    if iteration % 1000 ==0:\n",
    "        return lr*0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "adam = Adam(lr=0.001, decay=0,  sheduler_func=None, bias_correction=True)\n",
    "l2 = L2(w=0.001) # L2 regularization with lambda=0.001\n",
    "model.compile(optimizer=adam, loss=loss, regularization=l2)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_results, val_results, test_results = model.train(\n",
    "    mnist, \n",
    "    train_batch=50, val_batch=1000, test_batch=1000, \n",
    "    epochs=2, \n",
    "    val_intervals=-1, test_intervals=900, print_intervals=100)\n",
    "print('cost:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Train your best MNISTNet!\n",
    "Tweak the hyperparameters and structure of the MNISTNet. The network is small, hence the training should finish quickly using your CPU (less than 1 hour). \n",
    "\n",
    "The following cell should include your model e.g., `class MyModel` (similar to MNISTNet), the training and validation code. You just need to include the model with the best performance. When you submit, the execution results (output of the cell) should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your new model; and do the training and validation following the previous cell. \n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Task on RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## GRU\n",
    "\n",
    "\n",
    "The GRU layer calls the forward and backward functions of the gru operator in [nn/operators.py](nn/operators.py) to do the real operation.\n",
    "\n",
    "You need to implement the forward and backward function of the `gru` operator\n",
    "\n",
    "```python\n",
    "class GRU(Layer):\n",
    "    def __init__(self, in_features, units, h0=None, name='gru', initializer=Gaussian()):\n",
    "        \"\"\"\n",
    "        # Arguments\n",
    "            in_features: int, the number of input features\n",
    "            units: int, the number of hidden units\n",
    "            h0: default initial state, numpy array with shape (units,)\n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__(name=name)\n",
    "        self.trainable = True\n",
    "        self.cell = gru()  # it's operation instead of layer\n",
    "\n",
    "        self.kernel = initializer.initialize((in_features, 3 * units))\n",
    "        self.recurrent_kernel = initializer.initialize((units, 3 * units))\n",
    "\n",
    "        if h0 is None:\n",
    "            self.h0 = np.zeros(units)\n",
    "        else:\n",
    "            self.h0 = h0\n",
    "\n",
    "        self.kernel_grad = np.zeros(self.kernel.shape)\n",
    "        self.r_kernel_grad = np.zeros(self.recurrent_kernel.shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function of GRU Operator\n",
    "\n",
    "In the file [nn/operators.py](nn/operators.py), implement the forward function for `gru` operation. (`input` is a list of two numpy arrays, `[x, h]`). \n",
    "\n",
    "The following cell test the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from nn.layers import GRUCell\n",
    "from utils.tools import rel_error\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.random.uniform(size=(N, D))\n",
    "prev_h = np.random.uniform(size=(N, H))\n",
    "\n",
    "gru_cell = GRUCell(in_features=D, units=H)\n",
    "out = gru_cell.forward([x, prev_h])\n",
    "# compare with the keras implementation\n",
    "keras_x = layers.Input(shape=(1, D), name='x')\n",
    "keras_prev_h = layers.Input(shape=(H,), name='prev_h')\n",
    "# keras_rnn = layers.RNN(layers.(H),\n",
    "#                        name='rnn')(keras_x, initial_state=keras_prev_h)\n",
    "keras_rnn = layers.GRU(units=H, use_bias=False, recurrent_activation='sigmoid')(keras_x, initial_state=keras_prev_h)\n",
    "keras_model = keras.Model(inputs=[keras_x, keras_prev_h], \n",
    "                          outputs=keras_rnn)\n",
    "keras_model.layers[2].set_weights([gru_cell.kernel,\n",
    "                                   gru_cell.recurrent_kernel])\n",
    "keras_out = keras_model.predict_on_batch([x[:, None, :], prev_h])\n",
    "\n",
    "print('Relative error (<1e-5 will be fine): {}'.format(rel_error(keras_out, out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function of GRU Operator\n",
    "\n",
    "Implement the backward function for the `gru` operation in the file [nn/operators.py](nn/operators.py). \n",
    "\n",
    "When you are done, restart jupyter notebook and run the following cell to check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn.layers import GRUCell\n",
    "from utils.check_grads_rnn import check_grads_layer\n",
    "\n",
    "N, D, H = 3, 10, 4\n",
    "x = np.random.uniform(size=(N, D))\n",
    "prev_h = np.random.uniform(size=(N, H))\n",
    "in_grads = np.random.uniform(size=(N, H))\n",
    "\n",
    "gru_cell = GRUCell(in_features=D, units=H)\n",
    "\n",
    "check_grads_layer(gru_cell, [x, prev_h], in_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional RNN Layer\n",
    "\n",
    "Vallina RNN operates over input sequence in one direction, so it has limitations as the future input information cannot be reached from the current state. On the contrary, Bi-directional RNN addresses this shortcoming by operating the input sequence in both forward and backward directions. \n",
    "\n",
    "Usually, Bi-directional RNN is implemented by running two independent RNNs in opposite direction of the input data, and concatenating the outputs of the two RNNs. The following function reverses a batch of sequence data  which is necessary for implementing Bi-directional RNN.\n",
    "\n",
    "```python\n",
    "def _reverse_temporal_data(self, x, mask):\n",
    "    num_nan = np.sum(~mask, axis=1)\n",
    "    reversed_x = np.array(x[:, ::-1, :])\n",
    "    for i in range(num_nan.size):\n",
    "        reversed_x[i] = np.roll(reversed_x[i], x.shape[1]-num_nan[i], axis=0)\n",
    "    return reversed_x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function of Bi-directional RNN Layer\n",
    "\n",
    "Please implement `BiRNN.forward(self, inputs)` [nn/layers.py](nn/layers.py) and use the following code for testing. Note that `H` is the dimension of the hidden states of one internal RNN, so the actual dimension of the hidden states (or outputs) of Bidirectional RNN is `2*H`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "# import importlib\n",
    "# import rnn_layers\n",
    "# importlib.reload(rnn_layers)\n",
    "from nn.layers import BiRNN\n",
    "from utils.tools import rel_error\n",
    "\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "x = np.random.uniform(size=(N, T, D))\n",
    "x[0, -1:, :] = np.nan\n",
    "x[1, -2:, :] = np.nan\n",
    "h0 = np.random.uniform(size=(H,))\n",
    "hr = np.random.uniform(size=(H,))\n",
    "\n",
    "brnn = BiRNN(in_features=D, units=H, h0=h0, hr=hr)\n",
    "out = brnn.forward(x)\n",
    "\n",
    "keras_x = layers.Input(shape=(T, D), name='x')\n",
    "keras_h0 = layers.Input(shape=(H,), name='h0')\n",
    "keras_hr = layers.Input(shape=(H,), name='hr')\n",
    "keras_x_masked = layers.Masking(mask_value=0.)(keras_x)\n",
    "keras_rnn = layers.RNN(layers.SimpleRNNCell(H), return_sequences=True)\n",
    "keras_brnn = layers.Bidirectional(keras_rnn, merge_mode='concat', name='brnn')(\n",
    "        keras_x_masked, initial_state=[keras_h0, keras_hr])\n",
    "keras_model = keras.Model(inputs=[keras_x, keras_h0, keras_hr],\n",
    "                          outputs=keras_brnn)\n",
    "keras_model.get_layer('brnn').set_weights([brnn.forward_rnn.kernel,\n",
    "                                           brnn.forward_rnn.recurrent_kernel, \n",
    "                                           brnn.forward_rnn.bias,\n",
    "                                           brnn.backward_rnn.kernel, \n",
    "                                           brnn.backward_rnn.recurrent_kernel,\n",
    "                                           brnn.backward_rnn.bias])\n",
    "keras_out = keras_model.predict_on_batch([np.nan_to_num(x), np.tile(h0, (N, 1)), np.tile(hr, (N, 1))])\n",
    "nan_indices = np.where(np.any(np.isnan(x), axis=2))\n",
    "keras_out[nan_indices[0], nan_indices[1], :] = np.nan\n",
    "\n",
    "print('Relative error (<1e-5 will be fine): {}'.format(rel_error(keras_out, out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using RNNs\n",
    "\n",
    "In this section, you need to implement a RNN model for sentiment analysis. The dataset, `data/corpus.csv`, consists of 800 real movie comments and the corresponding labels that indicate whether the comments are positive or negative. For example:\n",
    "```\n",
    "POSTIVE: I absolutely LOVE Harry Potter, as you can tell already.\n",
    "NEGATIVE: My dad's being stupid about brokeback mountain...\n",
    "```\n",
    "\n",
    "We provide a basic model for your experiments, which can be found in [models/SentimentNet.py](models/SentimentNet.py). The architecture is as follow:\n",
    "\n",
    "```python\n",
    "Linear(vocab_size, 200, name='embedding')\n",
    "BiRNN(in_features=200, units=50, initializer=Gaussian(std=0.01))\n",
    "Linear(100, 32, name='linear1')\n",
    "TemporalPooling()\n",
    "Linear(32, 2, name='linear2')\n",
    "```\n",
    "\n",
    "The input to the network is a sequence of one-hot vectors, each of which represents a word. The 1st Linear layer works as an embedding layer. After a Bi-directional RNN layer and another Linear layer, a TemporalPooling layer (see `layers.py`) is used to aggregate the vectors into one vector, which skips `NaN`s. The rest of the network is the same as a normal classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import datasets\n",
    "from models.SentimentNet import SentimentNet\n",
    "from nn.loss import SoftmaxCrossEntropy, L2\n",
    "from nn.optimizers import Adam\n",
    "import numpy as np\n",
    "np.random.seed(5242)\n",
    "\n",
    "dataset = datasets.Sentiment()\n",
    "model = SentimentNet(dataset.dictionary)\n",
    "loss = SoftmaxCrossEntropy(num_class=2)\n",
    "\n",
    "adam = Adam(lr=0.01, decay=0,\n",
    "            sheduler_func=lambda lr, it: lr*0.5 if it%1000==0 else lr)\n",
    "model.compile(optimizer=adam, loss=loss, regularization=L2(w=0.001))\n",
    "train_results, val_results, test_results = model.train(\n",
    "        dataset, \n",
    "        train_batch=20, val_batch=100, test_batch=100, \n",
    "        epochs=5, \n",
    "        val_intervals=-1, test_intervals=25, print_intervals=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(train_results[:,0], train_results[:,1])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Training accuracy')\n",
    "plt.plot(train_results[:,0], train_results[:,2])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Testing loss')\n",
    "plt.plot(test_results[:,0], test_results[:, 1])\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Testing accuracy')\n",
    "plt.plot(test_results[:, 0], test_results[:,2])\n",
    "plt.plot(test_results[:, 0], test_results[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Train your best SentimentNet!\n",
    "\n",
    "Tweak the hyperparameters and structure of the SentimentNet.\n",
    "\n",
    "The following cell should include your model e.g., `class MyModel` (similar to SentimentNet), the training and validation code. You just need to include the model with the best performance. When you submit, the execution results (output of the cell) should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your new model and all training codes here, like loading data, defining optimizer and so on\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marking Scheme\n",
    "\n",
    "Marking scheme is shown below:\n",
    "- 3 marks for `RMSprop` update function\n",
    "- 3 marks for `conv` forward function\n",
    "- 3 marks for `pool` forward function\n",
    "- 3 marks for `dropout` forward and backward function\n",
    "- 3 marks for `gru` forward and backward function\n",
    "- 3 marks for `BiRNN` forward function\n",
    "- 3 marks for tuning your best MNISTNet\n",
    "- 3 marks for tuning your best SentimentNet\n",
    "- 1 marks for your submission format\n",
    "\n",
    "We will run multiple test cases to check the correctness of your implementation. You may not get the full marks even if you pass the tests in this notebook as we have a few other test cases for each task, which are not included in this notebook.\n",
    "\n",
    "For the tuning tasks, please use a fixed random seed (e.g., 0) to make sure the results are reproducible. \n",
    "\n",
    "As for submission format, please follow below submission instructions.\n",
    "\n",
    "**DO NOT** use external libraries like Tensorflow, keras and Pytorch in your implementation. **DO NOT** copy the code from the internet, e.g. github. We have offered all materials that you can refer to in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final submission instructions\n",
    "Please submit the following:\n",
    "\n",
    "1) Your codes in a folder named `codes`, and keep the structure of all files in this folder the same as what we have provided. \n",
    "\n",
    "**ASSIGNMENT DEADLINE: 19 Oct 2019 17:00**\n",
    "\n",
    "Do not include the `data` folder. Please zip the following folders under a folder named with your NUSNET ID: eg. `a0123456g.zip` and submit the zipped folder to LumiNUS/Files/Assignment Submission. If unzip the file, the structure should be like this:\n",
    "\n",
    "```bash\n",
    "a0123456g/\n",
    "    codes/\n",
    "        models/\n",
    "            ...\n",
    "        nn/\n",
    "            ...\n",
    "        utils/\n",
    "            ...\n",
    "        main.ipynb\n",
    "        README.MD\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/jupyterlab-fonts": {
   "fontLicenses": {
    "Anonymous Pro Bold": {
     "holders": [
      "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com), with Reserved Font Name Anonymous Pro Minus."
     ],
     "name": "SIL Open Font License 1.1",
     "spdx": "OFL-1.1",
     "text": "Copyright (c) 2009, Mark Simonson (http://www.ms-studio.com, mark@marksimonson.com),\nwith Reserved Font Name Anonymous Pro Minus.\n\nThis Font Software is licensed under the SIL Open Font License, Version 1.1.\nThis license is copied below, and is also available with a FAQ at:\nhttp://scripts.sil.org/OFL\n\n\n-----------------------------------------------------------\nSIL OPEN FONT LICENSE Version 1.1 - 26 February 2007\n-----------------------------------------------------------\n\nPREAMBLE\nThe goals of the Open Font License (OFL) are to stimulate worldwide\ndevelopment of collaborative font projects, to support the font creation\nefforts of academic and linguistic communities, and to provide a free and\nopen framework in which fonts may be shared and improved in partnership\nwith others.\n\nThe OFL allows the licensed fonts to be used, studied, modified and\nredistributed freely as long as they are not sold by themselves. The\nfonts, including any derivative works, can be bundled, embedded,\nredistributed and/or sold with any software provided that any reserved\nnames are not used by derivative works. The fonts and derivatives,\nhowever, cannot be released under any other type of license. The\nrequirement for fonts to remain under this license does not apply\nto any document created using the fonts or their derivatives.\n\nDEFINITIONS\n\"Font Software\" refers to the set of files released by the Copyright\nHolder(s) under this license and clearly marked as such. This may\ninclude source files, build scripts and documentation.\n\n\"Reserved Font Name\" refers to any names specified as such after the\ncopyright statement(s).\n\n\"Original Version\" refers to the collection of Font Software components as\ndistributed by the Copyright Holder(s).\n\n\"Modified Version\" refers to any derivative made by adding to, deleting,\nor substituting -- in part or in whole -- any of the components of the\nOriginal Version, by changing formats or by porting the Font Software to a\nnew environment.\n\n\"Author\" refers to any designer, engineer, programmer, technical\nwriter or other person who contributed to the Font Software.\n\nPERMISSION & CONDITIONS\nPermission is hereby granted, free of charge, to any person obtaining\na copy of the Font Software, to use, study, copy, merge, embed, modify,\nredistribute, and sell modified and unmodified copies of the Font\nSoftware, subject to the following conditions:\n\n1) Neither the Font Software nor any of its individual components,\nin Original or Modified Versions, may be sold by itself.\n\n2) Original or Modified Versions of the Font Software may be bundled,\nredistributed and/or sold with any software, provided that each copy\ncontains the above copyright notice and this license. These can be\nincluded either as stand-alone text files, human-readable headers or\nin the appropriate machine-readable metadata fields within text or\nbinary files as long as those fields can be easily viewed by the user.\n\n3) No Modified Version of the Font Software may use the Reserved Font\nName(s) unless explicit written permission is granted by the corresponding\nCopyright Holder. This restriction only applies to the primary font name as\npresented to the users.\n\n4) The name(s) of the Copyright Holder(s) or the Author(s) of the Font\nSoftware shall not be used to promote, endorse or advertise any\nModified Version, except to acknowledge the contribution(s) of the\nCopyright Holder(s) and the Author(s) or with their explicit written\npermission.\n\n5) The Font Software, modified or unmodified, in part or in whole,\nmust be distributed entirely under this license, and must not be\ndistributed under any other license. The requirement for fonts to\nremain under this license does not apply to any document created\nusing the Font Software.\n\nTERMINATION\nThis license becomes null and void if any of the above conditions are\nnot met.\n\nDISCLAIMER\nTHE FONT SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT\nOF COPYRIGHT, PATENT, TRADEMARK, OR OTHER RIGHT. IN NO EVENT SHALL THE\nCOPYRIGHT HOLDER BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\nINCLUDING ANY GENERAL, SPECIAL, INDIRECT, INCIDENTAL, OR CONSEQUENTIAL\nDAMAGES, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF THE USE OR INABILITY TO USE THE FONT SOFTWARE OR FROM\nOTHER DEALINGS IN THE FONT SOFTWARE.\n"
    }
   },
   "fonts": {
    "Anonymous Pro Bold": [
     {
      "fontFamily": "'Anonymous Pro Bold'",
      "src": "url('data:font/woff2;charset=utf-8;base64,d09GMgABAAAAAD7MABAAAAAAjlgAAD5qAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAABmAWi2AAgxYIMAmCYREQCoHwUIHcTAuDMAAShngBNgIkA4MwBCAFgwoHg2UMgTUbiIE3ZG5OOMEr3QnQqlZ98WFHIY0grbQG3BnYOEPegHdk////WUmHDAXaJSm0qs7N/Q4GIzwdTJAGLTOZNWdZubWZ5KYryu1HUgjLn4kufAVF8KKtjurKS26JK1C+jQGbNzSWXw7NV5wePWDopObz3XJHn9f52IW2L5oZ7B4d8eimYaBxHYzyZJiIpsHR4O+5hv1MBuy70en6uqP+MjPtqao5h/Ef0ab6IWB7nQa6ckuHG3llph+jgvNhOJYxQJfp+dhGP0wagSREI1LfFTAmB2JFgmPPM7Bt5E9y8kI82fXV6+6Z2fmwDoxoRmAshMbuwZGg6HqA32bP6py3OXfDQp0iMgORUEowSIkSJSwEE4wkxMbC6kXdzeFcaS98b5UWepu3v3+V7q69iklVQ5AEG4hGEmwgCbGrVdvSy+9vfUtpSmvjnC3TXD397T8suUy5KFlDhNvuGyMxYrM0S7CEpJPf3Pqeo01bHknzup3uv26b+UKdRnztWBCGBWFYMC7j6/fOOSOB7l4t6ukkQ8oNSfgJU1L619Wb3Wszf0ZauMxItvVSFmmKaLWGOzYdwlqNNmFvaoYPhF2AfA2O893/UI68/Dt5jtQKuUpSGCnnDmNHiE8lbh0A0+/n/kohNki0ylECRxOf+XedKaYze1MfnBdaPoDBJwf0dl9JExQosNwqQTqME12Y8txzzI3/6E3wBkIPTFDxxQ/BYREctq0K4WFzmnP6h9xh+azozop+GGaQdmcQ8ABwf8toCYxpi3bJvkG0AcS0BYVwhzBkhUEoP51uTjeNDkpArJCtKvww/rAPNvXtp5qpOJ2DwAD8AdfIoB7HpfrpuOvPWMfvXLw9x9N/UXqBgSdgAjYovhcM6YQ6jB2WzeOHA/0TF9N7B7aAHwQPm4AKzLKwZgLL6k2tz83qJrDArOrH3p2vhSXD35kvlCXu/HPM7uoGX+M9AATQ4AhgBbc5klxFcCX5BRsNbgwB/gnOfM0TSnuNxxOcOTUzJ4S2hFDWfu+UpwxLmt/cs7Xln6UsoSFcW32RrRn7LacDwY99cb7GclQCVFSm7PrJy46xmSv2UUHqIZEiLMwWs1lSEaNOg0yQyt3jZ5VjuKzJnqZ90ZzaSgSzgSGE9ge0GaBu4ICzzokEIoVXn+otzM26s025AV8rhWMWQTHW5OSr8Y/BAIGD5G1rxX+Fu0JM85ebAXqHAwMCDp6CXL0hgn/DQgE8u+Pqy4Xw87aFCUMIR4nAiMTty99GwFVoP+ENkQPFW0tApdxk3Ekecp4VWyl5UWBGvFV8qrfU8NU6qOOn528Ab7yFiQBTCDOB5pAtNrd0yEqQtWAbIbZC221mD+UgzA20W+HdbeoI4wTrCZwzvHsRPd3Eg0jPEDxH9AKplxt7hew1ijeivEX1Dq33G/kgmm5ACVgGVYI3RKrQGqwObyCakRtQLXQb08F249bnewSACJIgMpyynorQUDrGwDPXtQg2yaG4NI/JHwhYISfixULJOqkok+SyQtFXG6w91Ix0Y8PEbLrWzDK3LRxL18prveZGs9XtDHuzwxpHy8l2dlxcV6/b6nffI/AMvaLeq31i38Qv9c/Y6iLvctWrwnXppnJbd7fKfeOh9QiYIQvKut+G2QkH5WS59rk5j+CVfIq/K7A32IMHyAidwvbgM2JBrqgNvcfssgfuxF+EO3FHesgv5aP+NHP6tmERsgrbROxFt2IOcaeES9It5Sk9M15Zn5xfPtDT8hz0AntFvKHesT42P3FfhG/SD9Xvxh/tn1FgFTklvvKqIqiKapK6XGM0FS1VW9PRAaPuTC+kHzaIGEYbTY9jJnHThFnSPNViapm2ylhnbXJtJ3d5e/mAGqPc9C2Bd1nZKUv+XwJuJwEgkjz1/jPVkK7qkfq/ei9HVLJSVVMN1ZPqNV3OS//fQ7Qcgd4ak0KXZ4RVWhVXnZaLnQK2pL8x5z3+l9vmP9MDPBvCoFzO/plh/sNyfD5QAFDEFbthwOLfK5rIg+NGNGryqNfjZp3a9TvisCdt7lr1eLrYoU+ry+4vDThqeWV12AkzppykzeySNSd72qxr5l1xlSMHcN2CU3LXut0EuiFv/W0bvS6/sKBoUHFpSVm5ocJoema2VFbVVJ8zpK62vuH5xji7MRNGnTUJxfFUotVdAJctdX3zXTh+ZGc+hNT/PURvgzmQhg64A/idbC6SSSGWQDVyX5+OHqSCSH8UZJAHVQEpYw+6ggx0oZlbRyjKiVrfoxxD78sdpvDZ353wiHoHBBb2Ez9waRmNpPbW/xBpezA8bNOMenOOvtT21zfP9ffgVAzyddHUziwWqRoaKX0PZtIIrZsIuQ9gEQ+ufH538+9TlhRe0y0ztQBztHCjRuYYEi3/NO8wFrkna1G+XB4ktGMhbA5oNA9dqL140fTXaFGtaisv8bwfK1JjUR78isHCalVjFq8wCASNjyISsXsFwF0A2Q0iEHOVICkvoJaC7IQGk4Cjq6txImvg0FDuktJKV0rJLqOutQwU1Msq4am0O4nhmUGKSk8zBvLA8JABwxS1mYbJhFTXQyh5cBYe9HqpVb1Np7SRsNIkq0QsgkxN5AXTy5TDb2ABJ7YNBRZ44lEYxwGG+cWF64WjG2YMLfUnWLSUrf1QtZKcAn5nzvCJO8dPX4xWGAfYIkgEnlsH3H3CpU2KiDrQsSFsaUT6hEg7r5mAKIQYtUKIiERE7qSwSRHSKZwWiYEPKyFye3nPBIXuEyG4K4WAEHIjRxjDLGopFBje2qTwUCEHrwBPx6KwHckCp5QDrD7Zi6g+iqz0dqCGqrg8SDY+9D1D1k7jGV9JgolbexGBPndYvTnSqfIEhOurka9mKAFNVkiwMHE8/8DkxH8hVPr1yPBhkdkJFy9HIBiepoBXy9VzH/tDwfbJI+LIJZ7Q04dI9f8RPHb8tbM+SFJpVCo8Xrn4XyS2Cc2s+7UyLD+myfeu7X3jC7qVb+nTrwaCvZQutf3itadeaUST2qsBqM/op9P85b1W9j7vnS/dmDy+8A8IwRjqjv+wP7+UzS/YxRZ15t3SYr7QZq8ESYVastCbQ5qTaKBOyD47crPQm2y18dKj4WB1YkQl736wqTXk+P6/Gn+Y5yn2ejXtamTRwF2clluSHzNApaiCCOF5a/63QlkvsdL9Rp1d2McKrKHnxG479uMhD8n6GaMUG6iJnqRZY2aheEkdeqjRAeNgBVTFYRTSX6bjX7/8+hqQNOtOz/+vaenov/o2HozURu1zD5Knp0Zf0uYtPH8hszRtqIuQ8vR5Qk4qOrzQzJDmvO0Psq5voImwYtDPe10HrG70JUev3fL4BUK++PzmvhEIb498iRvB+Pbj0zRkER6S6jndQLjxukyiCvYM8Kdau+vzufaXFySlp/xX8/trj0xbNclzsaNnfTBYea1iRLXexDUmoWS8sXxjHUuDSpN8wlnIgiCo9fJLcYpSU8LRvRZqQoMSdf+8U0TeUsPz7FOoDcUMixzpmL1rPiWs+82E/Si461EzZdH0ihUXHOihaXieqhtKm9ROtq2ZubPqaPjIdZFyzFT6kxpWQUNdimjboMYYxKQo/uJ7Ao8nYIyf8Lg5Va2sK6vZjmFngus4YMk0AdreP9GLbkNhwCYvF22fZVcKkpQwaGKECPzbUk4G92fL/RGNuPBte3b6dGKHW94o5HaTgS/eaRNqoFZnZbPFnoaC6za58mvPNDADHTVw5FygUF4DFVzYUb6tPP/yy/S0MqRVBjTge3axLfemKj9ncx2KKIH0TnAd/H9UH4Fg8c4ai2sB57+tQ/rzNvrsUNFJOg0+AkP4GufTWiMJ9fTvET/X0omZwcEXg/O8BepVpHwUm5hLBVrwvzm+YkCbOOY5qMHDdJEtkrKgpRY7LQjIAONe4Am54L1GR9wb8/wvQfxZJaL5z55R/SBGUu+Ge1dPMa+pV/Klk/LkqPOu6jBY6texEGvj6LS3o2CwwOsImvugFiTVZTTnfxbYAPPVtATFOllK2o9csqTyvT15ni9QsOyDtPcKSciSJ5BayDQxRy/HkeQkmDQK8tLMHJRSr37Ay1dz9vos10csE/vz7yyp1ucjnpmLpw8VOHsaKz2Hc16MJMQcBhp/AREMfdKStjd9JbxOnjOuvv/d076G/Ng3TKhHhqvAGoHqPdqtSn+4nImMylYJVn9QUvu92oKmaudnd3FUux4JrcOQ3mGc5NDB5MPH0BZO5EZLWhqKo6G2rQ5jFP3uRJE8bsmuNOI2WTzbFs2O5bWCdo5HFnjbI1KRvf+PFXL1+flthORL3/K7GowvEymA8ZG1/mahJpE5t8M+QR8Yf71d+ih2+4gRXVBMEcUTd1qwXAuWyW8XRkxYTr1pqBN/UmPD75BQyq8LYVGihyWtHkjJEVA82yoXHIBzxwVjxUAq+ONZWYrZ7rwbTBfV+WpeS2pmS3VQQi9O7HA8q/13y2F1RdOzeFpj52Qd1okfz2V3/7KtJrcv3VV0xYKTiQvGMQuJ8O+RhfHabmAcVqqnEzfv3i3K/Dd5Zf1Z3e1Off+sgSBP1rBuQJlTqluvRywh5k2q0+oUm3GLUtjx95zcfmxqSe1Pr569eo8Tff0wCE99PXV9mBwae6q7l90ty7s/mfTYsVA2MHN9c8tW5xYuzEDh7HFKMFC9bhejVR/JGppB8xfb9cf2obvQbsHtenUhprXVRuF7fCX6fGLysSk/Uwwk9I5j1emerh4vqHJ5KfJsgxUru2zas0wizpBqp6C/jSdhVUnEE44aSbZR65/fzWuRB86ljYutH5LNVLrJUgPNnlU55XSkTqAMdP06hOtpff2DiWS87kbIN2+g8kYG7V7pVa3Q7ubNIba6Kd2smTePeOuK2mCG2La0FAatxcW43eFDKpVNg1lrs9qzO+OoSMU7aVWqeA5RhSX1X/mGd/w8c5pbl//tR+vbQPPREMc1LXX6KMWMqf+/P7djurTsGPNPbqHyER4/ASolq44eVtqH6HF4r08YK1T+01r3zjNAGPxwWCHjdan655e/m1iwpkcOf1DFvnK1038cIId2qIM9y6bcIIABvPnq8CxnL27zb47J17E1qGX7uj3b77rdHT2omyF8y/WXRuHziAp4h6R6usFXKeSZswAJ0Bur3lHKcxsRzz+KLf/ZRioR1nCOgQkBjBlM0Njz5+NQ6SAINMCHCKSf8dJTVVP0k/em0XXCfXHN9dTtg8jbZ4OuMrzrozNTqBZ5w7W5MbeLwKTUpZ6Tkh1Ru4N1193DbkOZWd4tDJ2G0ZTOjDBlaNtjVXsuAmoXXpqKiygUuqhk375Dk4r/jWDtFTKHafZNAvFAu4qg8CTcdY8MyWAx00KIBHUIkzk9VyQxJI3Jei4CMS2ExbrnCClsOSzsvRTvP2TFrbmSEKJm3i2RIsEqqwnEKkWSiIlhvIdlrPwkfP+/2vM9oiuPegzDPdlvHRoe4DIuCfWJfWSdhmYVNWvHmFQKN6ZfLiLWqlRmDIttDlEJcYXxLEyBUKHmfu+ztxFZ+E7LbVoO+lwUbns8RhOtLCvcyaKR3VRMLh3n7MsMxf4RTuMoZpGfw9Bbp/l1gppJFRGfZq71LaRx6+sLksael/5I29XWRnut9Ae9Z0FSfX1afB+QuRbxCQ5HMGRe7C28DRM9z+jzih9orz1xV8WPo4Lbt5D7Kc5LArufNUEcJA40p2APfBSPfC/yV2KAOEj3d+FSGY0mI2A0gas9yVbgDczpBQy0EbADzgl/5w+G4saUx4TYL14Lc53h3c3kLdPCrbK2NpNTuUDcam4m/mS6MTU7Ovj/v+YbhJ9Wm9L3KP7TAlglScq+kW3I1eQyBb3TeypZvkf61z8B9ALxg9crgLe/WGMLtRFm7zgH24rOTYvtzHRo1lWUofn0zNsKxZG2hjmwmHff5VcWJ7o6KjKIvyp2C67p7uu59/6rP/7Jj+GManafIaoxS8npNGYeJco1o5G1OvFZp7K7/s7WoWFTllmnNB3JvgWyKmCKSBwPn5iZk1HT1DL40MCYMBnHqLny/ogiTXSbkozIpAu1mLOsXboorgpZesVuANZHt4MHn23/afsGuDG6cSZ9H9dFRv34qlY83lI7wdVmvEm26PhHnDIf+DqX1XfWxGWh2HxkdlSzbaCUa6EhmI4VPJiVmGaZlh+hl5tZlzppjBtF8ZdqLRcYmUO3yooX0FvNrfStthtPi7uzJqj0qNLD6CIkQ97MUlWjx3P8dJeae+RHuBW1zInVvBrbWEPN4NG6WLiOKdWj+aW9gyCVnVHdcuSbpP9Gs213nm27gkZnvv7mT2dqd717/bCBQ71bquP3WzUT4Vk5l8IbNbx+HTGokJeuj/4yVxFFlcLTwpLZNnPKCbRcfgJtSuHYksPgaVSpKsr0my6anx5S+I+n0pilzzdkKRML1eXlhWrMdzSUjB+h8SCTtR68CJSMRkbJeEStJ5ms8eQTUbJn23ZsKR7fC9K1wzfLSkD0j8/N2LKBSyWdz6AxKGVH9jUxXyz3oP5SU6/88GNXcmvanr966FhdDFz/IEFJ3xAoHf87rdqVxr7GuK8CcnlY5FqyqfI/PdGPbDZnKjPaDtixut/12w/bbGbLr/+UxqEz5dk3AkKJWK1OEqcQijgbiQSrTJemkikIKSSGiMPMSil6XXPb943xe3enzy1MQZ83/P3334XRn1xme3RRyjRcgQ/78Aj8devul5XwmMQovMa9ii6A2+h56uiYA2leY3rZme/hwjc8voSyd6cT8oNvKvu65Tm53UrlSNCiyRD0VDnSrcxNtEYlUDMbrA/S+yNK0iR1+BGPcVWK53n8iKSuNLUvIi3V/fZMkVRF9HqePOnvjoX5za3uVeBJe3ldeE/Y4AtXduB/ylGTptNIL2Em2HfqX9mpb5lvwnDUgLzZH19/+BYkhnUknvKu5YtbCrJzzuYG1gZVd/0xFoXisoKS9oQBeiAsaQ8rCMVF/5LVrq5Zb1yvac/ooTX2eWqkJyqKj9KyO26WWW4QfmpuJmyZF66VtYap3IVoCgM4Dn4Hxhw0M9Kyo9jJpQpWl/dEcrL3FKOzXJHc0H4mpH08+qeeZtI3ltlb+W3nITHsfOutAssM5evVpk+Ofdfz0v6y59jWrrKxK2jQb4KbjYYYMiHjrU1z/GQkxVR0tBJ+tCzcKumAML1A2GpawOmOjmCZO38Bx3MfKuLIMjHUgmpqYtcTZ3wvQIH1CYODZuf3g+YItAc6tPsqgFX7KlgBbtg3Ysj4n35/e//Lya+vYq4scHyNTU2lLAUJl3bAwgx2xJ7+iWkUuX+yrYSMWH/kK4NrdmgMwQaHoen5W9bnhn+O/HpBXKz6vdjwg1fk7OyXz96dHlvNzY6sXPgmyd5T9M4yRfIUyNATeleFIjMaxqmsoQff3tr4eP/ZHpMvvn2wejOnKbv846GPy9sxOZz/SFqF+iBr7mhtyRvOWX281gNnPBfuMmCLf33wmsv1HKhNmQl5uWoY6gHA1ddkrhkn+XeVhCat2koSOA9W5/CU4cOhDw3tT+IqCj4vNbpbn9kfmZg+xCAIK4GLAB2dyWdmIsjqckUGHsGhk9QhV+M9e2ilKo41Sxlr1as60ZJUqxBjAg4DBos1L9WcJAr+4nJK0Dk+bIBh0PDb56iBMQsB03YHUASs2mcDrr83KiLj10txc3n1pzhFxcc53V64K6XxDh7eRY9KSaKwWemFwlUuBXieIyvmFxjMBPsGl/USKneGEpzxsUTY3jHYRkVEWcRG9PPxhVNzacYfj9L+cJorTbl+ERC0B7UH8YFzQPk///tjLgM8Dcpb4DZ4IvgWmPv74j/BWElryciA81PXgTJgLdWfnwGeAFPH6s/Wp4IndAbmdWTzuR5f8AUeDDTYB/yvKSLKTbOQlDcL/A549nsBlT5dDhcwxlnwh4t/mg2MjcB+FoYPxX82sPl7FlA8/03k3lo35JfcPrUVv4OsVVX3yq6HaInCHdmH+GXcdPkERoxOOpNGeDvaJBoMMIo5tenlKx044a2kErYFqeITiuoRMQV+MhJGjA10UmwLOHpcSjMt73CHOZ7aLQRJ6jFesTVuvqAwbq7YeoybKrGQ7UrH6m+9FSiiaXCsrW1w3ESsQPEeUPa3pqQ8YhhNtzgt9fFzpQXx8xbrWV66tkfqL2pbmF2osQ0WhWUgJPzI8gQJviyBV4Kk5Q7HJk0YHMr5KrIk9Ri3xBo7V1gQO19iPcZLlVQR5vfNg82rRTzjMDSOMRhX3bnZtE9ztL6R8adN/kBKyhNGZdECt80qmikuiJ+1Ns3xisuviZoG+G+3bl5xTTlWlJV1osg1RUNlbhZiN4AKYBW7Cq7e+XrxqR9rV+DVK/iETI8sNym5sI6Me13LGDHr3iJlb10azHyzt3qysElflvOryQg5wpKm2ImsrNjJEmiBbEq+8OJvLNnUf7YhLqiALdaio8hGNO+5wRFXLEuSGPoX7KOOVN/DzzJuq+k7s3DXemjnwb/zm/d8DgPxHXDq1QZb1bnRdLq648t8Ejs/uyJj8xk4ak+lDRTqjpDcc88XppWje8kWtQYm5usHMUa+E5dk+yuAZfsyeELzgmx3NDAnEbHwztYfhsffCozxey/b19vcRQopLzqB9z2CI2BKtHndfRdgvZvd8Qrl0u6uF7NnrNY5bmlx5XW57JniAva8tWkuvriUHr1pr5X8uKAyvlSPYTB1GK4Ue8Qxgwq5Mh2aydCj+TJ8Fo3vgNodcq/BEkoOgv7EyHiTY8n6Ke2mL/65u0V5+zFDdCCCYwlRyXDV8q/WAQPwDPsMHH0V0fXI0BGmfuNRJWXGM01WbuWc9hY1/MVPoykuFMddJaTXlROwnv/wvSxeKS7cPVZEZhw1UyAUldRyT/hLSlYpYoLsTTFuEJF8SOrE8njke6gJc8pDsacHmRtLyTpN/TjUdfDR5ZHENqJGQ7AlDD0Dn5kcpjawbShVpeIlK3KduJXKt+hFcadiwLhP2/SrHG+bANyxUll292b7z9VlvP/LyNYIPhpq333fDlkIs3GQ/kFLspLskndzStxmCpD/k5VYf6/2ngi4CrSt+T1yrQPtILNsrmJOAJ4Hqx65rvm1Vkbdt8OQqWjLpZTKULUrAnVzirvXVx0gQ6Z4sG6++G8E3Ak0MK8fwKlvXK3EglBM2cK14giZ6XhMXghPGJIbezFRjVMDajVGfjE2JJcnzEPFHAeOEwtIFzMqjDwXDhOBgovDLxILiMfpmAUAkvQArvXg6T2Ik+FwMcp9MIfvUm7MuEjKo2ezGJMbwucLnCBXY9KBdDUucRKP5QtzQ74urKJZIq7cteWBy66NqGPi0jKGMDFFTNOgxIFHVPaC+3atRx0Xl5XRv8lpkqgMtMTVtegiNkD2ngyBxcoQywgZNipCO+E1/9OPWsUfoqerozu2mtVdaWeC2cF3fwJ7ldbYhxqdeLff52A8pjJFZY1MnE1OkvI8zS5nLbprmM9q6pBfjnbK/lYud5ipcIbx+5RZdP/iv8VJbq/2DBG1PSfHRpWI2qnZOdODRBKqLTu7ndbN22g52czpFmKRSe8lIbHYKlqOq9jF1Are/++iBdp91/HXB/8p4Lx69xhTLMzvP19B4Tsn8F+voJ7P7xeLx1hqp/PA9HW446/F20Z9h8yHKPPVtzduAl+IiE1padUEVs2FJg8LgD5Ykh3FFaZ+AQIgwIzItF1ma/bzQ0jxjy+kMHp2dJ57CcQEfvXFtOe3gGEnK1xAsS1+RSpTZsR2FqX1oxMThlHVKlnLnrTbcNeKelWpwCyVsLRJvXHj0b0qbUu+Nsq6+WWUUZ7O6CvLHSYkKk+hmlSadjcNgPHV9VpaEy2aZIk2pTdmnNSr0nSu2GI4siKqDcf0e+rHILJZSYWUFhTL64/6BYyI1p6dbaNJRO207Gxau0hC05afrLJBh+5+++DFhxnx34v7Tu5zuI16PLpJMY6cHNm1yJcZeU49kYM2KMtCZeyNIRRkiKzhwuxeMoNgPENXISKEnNiw0EsKgp6gYFFT4gR0lZTBoOMPfc/O4RzpkzhEIq8L0t5hdY3IjE4QonSxifRMKZ0eifC5qMLkUlRSWsllzqf20qTFwaPLaVVFi9ITg+rlBoNq6ciRJZWp/HHSsSMpS3RzjnCmpW1OosubE7S1iOb1+aK5lpZ5oT53TtjSIp4DvnBUfRL46aR9b6Tam0sKUUQlUHNkSL+X4U9CM85nhN45S9ruphlcSbAR1FpCS4IQ16TOaqEoVzSD3VlomUGuLlUq08qUBrSss7wEiJ0uhtRLN13GMuUK25yUhK9nLl9vvos2Pp3a2tA+5OeMs5eoluGv+pFDdZEvCkgbHg9hlaBADwNhjQLgyLcNO1XV4xfZuqCjpbS0fLCMOzLbnN6z8YqWltTesf1t8RF7zayqsTtSardty+OUKfbZ50N3iSlO4lcg/PgYwY0sgPVP5K92wE4twuxMugIDH44bX3yE+gD14oLxPugpu7IzH8g3rZqKgeYRj+xdw+uQXbB1ZpFMJqnou2a3O+yOL/0qj7Yeh5yNiGdnGqMGYGL/Q59u3abh1VgR7yicgYy1PpnBGHhW70wqJY25Gyj+tFOQ39Zto+8JVXlwA1Y8mFzWtkNfFge27/pqE9gEsfdHlZSBvNx+ilI5QNLlUvpSkymDuryBpbdt5uVNlzOlqjA3ajMrGysMptrsLGPtSd2jn5XD95qC6HnJCTFp4XEC/xxyQkS1SlYeQvf4OCdaUB+OUCBRtWaS4pgghldK8eIiWNnR8RkF2aQ3XZ7mN9dx1OEBD45PRCEKa/V5B7+bFbh8ssOMkgWHYVJ2lCzM3J3xxAvZsZG9AWRaJvPvee0DeD2vgFuuc7LGon9uL1/UPvLXQUnHzJGcMRq/PmVffxkXfKUoVOVLZOjRXFlQYSSDiZgr4FDyQuKv+XoNLSAUizN2bF+iMH5DTqiX6XchMsVsRlZqobO2o3tT08tMO3dU2C9WGuI1X3kUle4CGjdB+4EnoLmSFc/imy2La9Grpvd7mixP11zHO1ePNVtYfO902/jqUrkeD1JGr1ObD9/mMfaXNYSBJuSoHUlPE3vHN7wEE/S/gr/eSPBaK3aag1pDuP7+IVxP6+654qLd855LYv9IYEOtTvMOilUbWUsMvXf8CsGnusOSPo9bRC2eh736EmZH30bdns4pyyeN7VvMWF1QI3VkClK3I31VF56aHo5NT/3fsan5NCuQ0bzXjOOm1+ORVlV2Ly3/crTU+yMoK2yfA/PXL67E+4fllxC/wEx7OJ6T00tdrFTlyl3xwq839lH1J7K+GjfSelGdxvEjWU3eM7acD5Q+/zuvfh58mklH7aA94Ajlj2znrsuuqrhjmoqmWHlifbSmJO4EVHMF6dWlsqUnqGzQrKuTT6HB7h7Bbk8nr0KzVS0JqQqbb/sVJFQTd0JTXE9LlDfFacrijrqmXJ486hbs4R4MPdpnBc2VzHimxWLnb60kmHzmvLNU83e+s3P4X/JphnobG/Nh676W94eejDfGSHN/XP5lSZrNitlLsr+P0diK0vEc5OI8ksCOdbJwMGpbkRrPRv7ffiiSE+ssxwy7Yaewzq6Fm7iAhPcrgzdaQSTgsAjZ8ifjewGyFkwQJziYExT8+J4WcUjBviN0OhjXeiw0qlM4dTzw0BjMb8X+XNglHiQ1XyxA9cnlqN4Cs9liMZmN0osXY2+iynsUhPTK7VI/skSROV8uOy/vRV01CT7uTyP0oNK2+AOW0hlBRQ8paqQsOP5eHqn+0vShMhrtoN3Z160uDMn9urN1uWGo4Z21/ZvuBx4cCHhFPfbVy0M8t+tori90iFWmT16CPjjs4rQu1m3RM2gSm5VACQ33/Cp0RP+wPz05VxUv+IRP0Jz6KrEh9dVPq4Lj23waIcrn9ExqSICcfzj9r2+kT7b+ZPmjI8+DemJSG1NPWPVSXRtt21AYMv69rpYVafL8/+8GnYPCX0Ud/xKftXzo+XCIo1zYWVLKuiMs8v2eb3UP/O3irlvGX84Fpsme2eYK0BLtHDkhsfKzqL8OXm+s/uReJzZ94+KSmHTBvGdfQ+dKJBEuX1BScj3z6MNvwY2PPTOmP83/0H97Z/l2Tktm+cc9H5e3aPL2Da2XFMPZvsWxcyOFeOFAv9lSUc6ONFYlDWEm9I3xq20Dt8VVKz+PXNdp3pqp/iPpqFe0SsMYGrLFcm+2pj2gPy/eEgbvdu36fr9QxrWDF1WEf4HtRSBA8FgOmWpyYQcVltnYQig2cUaThIv5uhw7bMwglzN0n13xumA84xm/b5mq2DiDYkkNuRN4EhszmMsFlVRSmUoLHI0PiJKK/1RGBGeQNhE36RR0A6PxhzJqVINcLucB15DBTpVUSZVUqS63nlrA9CDe/P0c43IyInYDtPJfBPRzt5sa3Y1T3R/6D9uCbenHh3zCULab11fu1BUqC7BCoFiy4PUtOXWFynvCOAkdRQnqW0p2ZnIctVQJsRNoqm9wU/NCZQFWC5ToXQ1Shco6LMXaD5Cpebx7dU77k6fqr1Kvv9ILP8jShFL9LoDrfsDTM4dCl3LeeKWHEqRqvFKtAhs6GtJ3at1Kjo7QFYJvcJiaUpOGeCXg+0y8XgBv7h9dvk+9/P3fIFAC3BMAoEdWu9dd46jkqOXQaxt0VS/Mh620pS3Y8PwQDBZ8GAih2xEyGChgdZQGI34tzySUUFJGgckiK+fOAcD5MMD6+Qj4XqhwkAOB2QFiYKTIdbFeXurQ7zXadKcn/g0VTmwLTPWngL9Rr3+cpFld9vV+UsYaMrvZXvTDyfLCibB2Klha4l7qXP8hrREgpFADB7qc36w9POUVdqi+Cnf1Dbkcg1zyx9dvDV3zJ3s3cfh7pwHC+dCgjLMKD98e+zvR2Ae/9bfO3nr4NK3HeDl/+Yy3y/mVpmDAEquMWcuCTemUrumRdg6DXUzdh8/+Jx2dvD4qFfkl77tSVpHalalBQdVu6fbu6jDfx3C94xbLpT5UsYXKXqwMBaU+LA/7w/WA3bni4803GkJNXEJuw/UW2qu9hcTUvhC5DU0ud22v5x8ykvNlN3VKI1FuUmUvRznXc73VZ41oL8L5B3rX7hy5y3Nes+Quf+Xv89P5+fxyHi/nr7lyu9yHe4MUzkfh+TXfn2iDAYY01IEH4zsbHxIJ2qg+Qci5x+v54zmtPFnu/lzZZsp7toww/Ci7NitkXmsyueR+B16dzT4aOidw3babhOnMbFYjhi7nt5MwqwszPh/gerTFQ+Q2D1lMwbIVLbj1Prw5J0DAWy/cgHrQuyAm1yZvsYUDXOAxMuh1InU8AkvnjyApYQIZU058YHpFqOnjDsRtnmVcZCt7KwI43rmMacqacqcwsUA5HZ0MtlrY0MmtqduQU3LyydsDYIvwYktvHH4HaTVtNu0nf/KXoVLupfK9LV1iTNpTsTMAAEzbqDooCD8yiQM9LLABAwjhYIiVsRyIbyNlt8k6t+l6qNd6r62ma3Tv1smDeAuFmMVEHM3Fw+QrZehKiFxZwoogklBBGZgu5W8qszSlL2MpJXTLKM2tE012XevjuzH9bjBzPYINKA5C50XZ3ZIO1Ha912U32go0wQiVHrm5GMnvi1VwsNVyDaUrTR2TB3MxOXOSRUlO0SEn8i2kCtfqhsiNYeU2D3nKNScjExXW3TlKrXNlilzZYhCr2IUJ4rT/xrOsm76cuiZNnimjpjWjMYOHNMCrNp8PboQzomHzYGvsMJwIFsx6TZk76c66eO29i4M9YLRSNPBCXQTmYmKJDQ6oSBeIjm66SdlEkaCQEgDZRZuAArcUzDoBgM5hAy4gPOggijIdA8YJqB0qlzvP+aR5zXXezse5zaGHgDAvZZ9dfxpUqtGbzVJ+8Pz9VwPY+YJGhbRqKW8l7+I3tkBAemoTJoVGrbRTVnDUNVE+qFQSwN3eu5ycx8XKLY9sjCzdlLMTCiKYihnuAOCrqx/AcrevG0gooYEBtjvl9t5q9GoVphRJUp36JAm6KZK//gM0+5IDyea3tshEnzP9SrpuQqfr85Xh8X2bOGLtPeCV00e5kor3hzCplmDo4CLvt/SK1/havXav/MoD59Vfm3y4BRGr/Y6Z4H6TW89PaNcrkX1knD/8BwTJDWcLbSJ2+vWOup3Y61tzd4113jrX5bpZD2tdgzmCV3s9d//6ImV9eexXUaT3E+crihRnY+vsrJ2XsbBcTf1l0VLVVkc1pYHFbmJw5Io7ZgaHnYdycnzaP4BnSZkgQKCGHgTAgQAvlY7QWrkJabeUqUxNGpImz3BbEAA1nw9tu6kpZCETJMh36a/IxSExtuwH/ipodR0PQ2XbkHjheAC3PyNqVOGRZ8rodAEyp+SyZB7LOOWJH2gEp7goH+vwHWUJlAoqWHhW4AaguwVFLQ00UaUbdJbqEN7ePj+hr42oQ+UAd5bI+/SjSwuPToeZrnhWzjpyuzM3OLg6W2b77JqNqdqVHjDCmMY68tiOMU6H6bAejgM4IyM7cvUdJGn6Cc5wTud65jM4P/FzeEdfil/omXomSecX5sP8SPv9pk3O/kTer3/NnB7951+pX/CfXo9eji/8d91fzz9CQhJ2uesMJmDYgoY+jGFez+ttfa6BZ/tO/ZWegjqaaSWhjr7o+/R0ej69nGCairJ0jACB4D0yyEhuFzTZde8X3KIkHY+YYUJFInW/nnVTyanr039hvrz61o/efCyp0e/4xFrOic5A3tLJgI6L1qXKiJs0Wt2WMZhj0ZjHUixuj2tHYZwqkghMy1SgiEVVdAUXsL/wgiKKhnZdQwer4xIR56THZmiSEJFaehGBnoQ4B8Q68feIYbWttd0rcnJv0/Jun0yttdHM0OTVjHWBICveyTnBBuerqoAJ8Y1seBSy3sFhUtjgQAfk22v0k8JCG1ZBI6foyKJNGzEWjgZ8xwrr4uqVv077aWcJs/cB7gNrEvuhcSbJSVZQODMentLkSjcVVJdtEQSnoK07CI3Uz2diQVhB0qY0PYEOFHvqgMHWJ/oD9T8+0bpMKPGx3/k7FweBwj1LB6LJaGesvXxyX3Y3ufQGXI12puBs0IanEgLZg8xSds6SGoLHrpAL0VlqRN54byZkTVmWr5kToQAxAdkeECAgDNbeyFYZnkPecDhfxlexrLYniQ4bQwB/K8cFC2s41BAObujgHZ+ZLE1ZcsM9j1w4OFlwVmLVarrbojG21dgmNKnRhgYIHTEBAhasKVN5i3C93CYv29N92h+2bfQyDHqWRiZ8MGBrKlXscWjs2VA82LWpPlVfLdVWcQU31F5Zc3ee06/0vKdhWqd9sgkoLsVpXUBHis+AJ6YgqaGeRioURBC0Fel3Zs5YG3eDn9029WN/wk/60R9ABrkps20lF56IWVReKb48l2eWxzsbsjXbM8vAxnOgGN5ebMbWjNBnG1Plx5nQ+I4Z8wk8DBw1Pqf/G+PXmEXqmDaY1M8pKo2LY2o3t2Y7iuWEk/lj8pMY4kkkQsDxgyRyrD8jtGp57y4x5y5rtnOY06wz3RDD1zmYLxEHonvbA2TuREdSv/9BPih7L2a4zjWyITOaGhmQjLnMntIRqQYOLXmZohDYDDzjzHukKU0owmxBxndn+8RCJu3gmmu0IJw2e2eVHJJhLFNXJFXDjFsQbChMedTW6JFZSvjeXOmrgm8hhECCVKN8VOUBwjHQOSnujXvCRUzh6DBFQDgFYPh/nFlqdfCSc7dlTMUtfkdZnQDVTIoNmqbcR1MZADS9Y5KjGUvIhEABbDaSi3WqReAwSEgCAJbMFXzZ6wdkFO0p3SLtUCquXNJTshaVnZshhdtPqcqb2tSiyzPRp9F6bxgy+IyPxYVMGWXBE1JlxcJ0C21aftsR9itsiS5G5OE0JnGM96EB9gW6VXlTm41pfBMbaaDbRpPBlT80nNImU+V0G2jQFpog7OrWNWuNNIemI+Slcifn5inz8paDXOUuTSLLKAIGbspKVGBiFoOp1AZcBcxThRYDJlREIt2qJS4wvulKTGkywzyLTBiINNjNQUyjLNTpMVwb1EbPJQRLPk0UxZ4uhGBycNFVrnPsYL9zR37RJ3I+IqL7Yia8k+W6FnaSTVvaxg5WLaY5Whh2baIgoZp6EoIeBSErXVIVIo7LdszMTqRH+KXwN5XkeyYERqywQ0Zw0JFx/tvVV5ybrhUO9u92ZQoRJNShDxJAhAjTuhkQITyHrM5Ze55Gx334p1r7zIw9GUKE2n+VkbCiaONapmO7ZNWrX7LAOY71dlMk3kwrl8k5bNujLO0apZjSeDptPKPy0MmZwwIkHSGwbDsMOJF/QIES99hY2yDQzG24gqxIW4r/R70Q8zlWItP4mN4sZjNsYP/YDY2KI8rH53IhArJ+M8w+gcCEby9Mi3TGeRedOHAW4RLR+19ayFXHRbdDa+qqtW1oU6ttvlx+21awnUTUDRFJyV3EKjL8mfUn8fF8yTKBSDif5q1dE5hkppjgiPdQwCXN7SO9MdqJv5RZWkyd06pwlxyABUykfVM8kaX1hBVuNNhWHYeGDRcvL3uecorobDmFWKzzyEt2Agzj51q7AKh1Ak46kyt+eLi4KAY3y6CkFkt1Gl4SwUCBFWx5JJaVQ1Xxx9cb8/QwK8ijJAOxbO60QR2BgPgPDjeuSPhCRcMAK+xgQFkSm6DpYxWwxwU3ZAShHFMeMaPyYpWbgZuy5JYHnrjykmbowPk+FShfNnjUUuDlSzEX6ow/+Ikph+zlIjfJEnbIXSKW4WLPQamtTCrQoMeIguCqQLlwXp+bguWlySwLLDFlkEMSrttF2jzmhZ5xlXyU0/FbqimY/bUyzusaWHlGuibruN4vXWiOdwE3bOmE+0KMVd+YK7lc4S1mGHb2HmyoujJ2yBYyOWeMM83O2I/LuI08wv6Zj6SKSrLHxiacxDju4l1IgIMjGM+pfvZuj263vjrbhS512sVLEaIDBxhhgRKxiOREtrRkalNjXACCr4Jx2FaAdBibhCVYkfzz+mYgAJDlfKWIpDdhTtimyUwwxh3eQQCOGdhZhLOx9Dre2ly8HZAfLRkOyI/aDNlbIabZ3nvJQ1l4rX+5bBgPqdlNSEfoA5DNuAR7WTLsOdPnYsGkcROu3MbutNz3w5aEFjLq0sVcpYaiY1AOdkMEsTUOCQNu5ewb4DQEGbGtUPEqZFyEhs1Xbe4T1prUUNY7+GJCgI+xBaoPIe8wYg+I8XYlQscFZs5YEL72g843TIbrLpLgwBOvnHnLg09ykqs8ZGvG4fCukmYvhV2lqvpkXKG41KqEFSR2YWIQELVaqr26KiRKh0plRNag9c/TsTso9kq2XZgUOA2s5whCQCJiRApK1TmCaY4e+tTXnnsQc+8xohvpZbLrI+SW3bQKuStTPgff4g6AvCHBAMIGCjim8hX43dt69JcgcpKQVmnqoUEnXTVrEL1r5IU1yX0yS2+dOSCmijkWDJOqFEag2IQKiKyIoY+CIBxQxvYxHicxiVUcQmSHHiFYEaYff3vAJwfXL72qRe3qUuPl+s8sg3NJDI78rNk1q98NFoiyThw9kGwZhE/+NncCXN9YBC0omcOeZlqVD3o0Bf0jpMGPhKVIW5+j9dhZFZYaqxEBhCqYMZusIrx1CcxAUpvY0d6btod0E325I0b3IUTNiyF8wuf6p+2yBznxmf90BpN0fN4EQ/cmp0k3NdmJpn2yCdKW9P1guscCS1rqwgs4974wn3SXW+Gm//XX2cF0J7ADotoClkqPgnT6KiBMtOBqynnF1dAUoIEmNbXhBkTgjUZigfluRX0TJIbv7eLIBxrmf+yIFJrjB2eP8f6yn5n2iBBTrJEjiL1H7O/I9chlpOmIqZweLiyw1WkqyENb3TjjLGiZzIGOiy174nGPjz5u+kykjcMjHqtYxC4usSedQtOMzlZNalKrOtTJ+c3vOU36JdxjYt6x0CLhjoZgWqn/Xt9M25kHMltd1ogHP3XW9syGqhZFkTjeOiHIduRtNcx+yNJ59PrlmlblrSe/e/MHD9bU1bLaV9dquNIjEDBhRcYWA6fjdFyPxxEcBNlJnefVN7ZPuJ92o9d8Eqc3DnY+87S07x73j5j4O/nZ7DOaiHGLGvs4xnk/77f9uR8v1GJWURsWh7J7gfeWYlP7j67SoJG/8X1yrgiQBxN2TGDBsYLaZHYlsk0qjlCu3YW6uxFfVFLqXYOomSFPz9Kl+hZ8Yjl2eVBOlpswrcZt0OY12ZoW263VFgtybUNvecruTX3oTs961aJB9KFFnHRyqfRxwnK4TsP0y+DYb+Zhw/bNHUUip+G31fzYMYHhCIHso3hp1EyiJppbFGif4zHkRE44Rq8UEQhU7DwRIUYkdxjZgSK+ndWmM07wyoZtfuPlMtAkGa/VSMSJB1hsS25XN5HeIJbOBb3O4Aa3xRvctmWtwd/BQq8yrWl4Fex9KzKt4MEnXz17uHnl3qzmtnqweftOCBq2L3GArWhtJUy7qJvFJpONinYAu5/jl0pTn/jR37t676PP83ZiR5WfCGt0uqcKDuheZipLSuE5Hl5XOuf5ihYqp7Qm36lkedMveUmRveb+ZXyZkznZkjMZCyF8xKe19/XkdBW4V7fGxyp0cEC8c6kv4B7jBc7HU1LafmV6hQIllVq4gHjlJYM+pHp0PfBAW2srCIyxtWemWbjOzW514kDMwt0ItmUWZQkhABf3GlFgIUCC7UU/EMuRgP0VbvVtpqdP+494jueYnfP2q/sBtGES1C4ciVtkYSq6GIq12Asraj5x+iXyQxOpC8rN2BPsM4GeTmc82Gk2TU+JlUimBRiq/fh5LSS6YYepydrUvo611CCaqGvT74CrFoNYwO1OmkU564JLTl1cFQyBU2IRT9kp4Lftw0k7TCzmwFu+JRMc5jzj6ylDySiqKDsLWZsy7apIG6nlZjJ/8v3OmIdI+Fh+PO44Z0unnHLGeND7w9Q/B/YGl3TfMbRuLndoMcM8CWvHOxj2Q32WZ3bkfYgQzE91JWVTdXSvF71p1uBUrrN6OggTHmpu0vluHQaTfbphA1uTu+v5Qw4pz9y8pJVBJqmSbgjP5zN9X0zwoO0JNw4H85AGqokPAajZYAOAgrFBviP3StKl3zt4N22TnvBgBlpMXChZI3cElTcqZVVQSakC43cYNJW7l/J196aU8AhFAScoiLCjYCA+NjmuSWQ8oY7Sq6RyfFmxloWJNFEqNYaOtFaWtBw31LjqwZUsleoXCpTQKjNsFB3CVbY8gwZyAhaQHKUiHGeH5ATvKCFqOA8DItlQRkgWPEwrlxvKWgVXMnpaBLfly/m30gvkpWQflF0Y5bVEpu4YLr/Mtn6aqZnWHC1cKXZCJ9jSAPwGXtsTk1NMPVwGTmIpJzzyPZXY8qxJOTdlhyi/1mueY/txBB9HXZI8eCt3xlNr5u0EyYTSkM76DLWQCBMO2CZfOizSzHMEkg6q9+HlDlrKtGPg2dcREMTbMV+Nzw9aZ0qR1eZ5ebPMBuIxR607y6SqZOq8cpIK14ldqpPaxI723tR6G20225zAOWgkRGy4Ze3kQ+0aFWY7yzX1Yclp6VfqURvc8F1V5SJ3eclTIZlKDivkZrgvMpVJ38Cq8GCEqelpeoO/PsRXLO4eB0GliVFvhR2mq9QpQs1jpbDBAeGWQ7KdE3FNMEr4ED1UKLulPByd1SoK9lF5a/By0BL02QYZ9NZq4b8NgxCuYxqLk9kyM9MsTBlabs+R8/n2TP4hHdi+UMGvKzEm59G38/mAbOJAQKq/+Lt/+c/qvll/RKru+DgSdnVlYyBHzFZUD1nTKNIMOHZzAw6GK13bu2GwacVyTLVdA50f8Rn7XsuEBWcuushX+ipV/UqFH91M/K3N3QP/ZFNe8ob4aGUSJYSBQYdhIqc1flVXF0Wi1BycZahZl7tzvRx+Lb8czUfP+4aPRj6gWBnFhggTMj/mapaieVuG/GUipBT7jVY2pCDSXA5zj+Rsw7UA0RPaWU9r231L1eq7Y4deT/wtrrzqWl0EUjCDKTlnabq0rBJVP//+tF6et97V3qAwqqtH5jKztdNo1DRvgD/McXn2l6UYx2ad20QscoMGjITWgiIY0pOEryTPbQ9mVs95cFD07Xf06Hnw58vuW2HnIbKbEVi0DBoLg8Wyj1dpbNP3w5ZW7VLSJCpSamkkIwRFYKzyR1iFllplK8R24BtMETHGKnaRIzjRI28trafJrxa9SVKXU7yfxkaThorW2qso/APT0EbwX7UCKW/2jyPVCQQDPArb9Rf460i4C+CvIxEegL+ORHpAaBnNFxZGYwOjo3+P9ADW8LSLvzkK+ercruEusEfnxDR4sWzDR8EIn5bdNAMuklZMzU56cJeYAeAxb4FtRBNFyQ17RQHXT5/Wjb22cKFkPiAYu7DtrIMjsc6l5SOcb91G3C6gBgXz3snuNK7XCuOAG9UQJVa3rfqyTGcv860W4aXdkCf3KiamMsRpAPhbF9jZUYitiB68sgXERINuNVLMgiBnblAEZTZ011jnaAbXUqMKme4LCQCUYe1p66rbDwitF6amFtfVTiCKMaUIAp4u2EFrk3bmUU1nP86u2lt7SQKlm8IAKYodCYL7VIUhI9zlJFp+6Ml/isCGpAAI6WBWuofumQ4yw3QrD8Gb3mUWJzrbeSsR1qdQzdmu+4dOHzvKxseMyvpZQhrzsxQB1j5LU9/RZxkKrFUdaeHn726XvETRVKkyVSrk09EzgguRLRScWKYKheCS5D9NVhump+Ei4IpelPDlJDwcDDiGIp4Jd6QNWjW3o9yq2V2ZA4OhhHSVQ20yqCZ4y0sxlSqKe3gY579ykWgUhFQkaJyw1gjHsE5n4XWtlHGLQX4aMsxGXde3f6N6h2pu2cBB4+6nMPynHhePAD5hhBNBJFFEE+MNB0C5cefBkxcYbz58HeTHH1wAhEBIhwQJFiIUShi0cBhYOHgRIhEQkZBRRKGiiRYjVhw6BiYWNo54XDx8uQEQghEUwwmSohmW4wVRkhVV0w3Tsh3X84MwipM0y4v+YDgaT6az+WK5Wm8AwoQyLqTqh2lex8NhR5xy2lvOGHWW3QUXXYqCTZoybcasuacdi2uuf8k3ohW33X/w8PGTJyjXl56ZvNKxtv7s+cU/7O+8+N/L/7//QRM5GE55Jzv54T/+7b+u/ueffPMvPvOBT3zhq2eakI/DvQh3Np9eUbpxATVNudnsVuWy33dqvrQjPrMBpknA7vIjYUkNonvPCNd1i8YeihCdeL5Ico0h9w9FxeB5ov1SQ+rfxrVceJW6TotnjY1Zx9esi0Mec3ltgPJGzC7jle/lecUAAAA=') format('woff2')"
     }
    ]
   },
   "styles": {
    ":root": {
     "--jp-code-font-family": "'Anonymous Pro Bold'",
     "--jp-code-font-size": "16px",
     "--jp-content-font-size1": "16px"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
